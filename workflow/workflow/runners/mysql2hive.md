mysql导入hive

### 功能说明
将mysql数据导入到hive

### 其他说明
hive 登陆用户为任务第一个责任人（portal登录用户)

### 任务设置
#### 1. 基本信息  
参考 [基本信息设置](/workflow/workflow/runnerBasicInfo.md)  
#### 2. 调度  
参考 [调度设置](/workflow/workflow/runnerCycle.md)  

#### 3. 参数
![mysql2hive](/workflow/workflow/images/mysql2hive.png)

1. 源服务器  
待导入数据所在的 mysql server  
更多信息参考 [服务器配置](/workflow/services/readme.md)

2. 目标服务器  
存储最终结果的 hive server   
更多信息参考 [服务器配置](/workflow/services/readme.md)

3. select sql  
SQL查询语句

4. 目标DB名  
mysql DB名称

5. 目标表名  
待写入数据的mysql 表名

6. 导出文件编码  
默认为UTF-8。mysql数据写入hdfs将使用该编码格式，创建hive 外表也会使用该编码。

7. 分隔符  
创建hive 外表，以及读取临时hdfs 数据会用到

8. 数据入库模式  
有两种模式可选，append和truncate  
append模式不会删除原有数据，所有重跑实例，可能会有重复数据。  
truncate 模式会删除原有数据。如果目标hive 表是分区表，则会删除数据时间对应的分区，如果hive 不是分区表，则会将整个hive表记录删除。
 
9. 分区格式  
指定hive 表分区格式。
通常和任务调度周期对应，如任务调度周期为天，则分区格式为P_${YYYYMMDD}。

10. TDW参数  
暂时没有启用

6. 源文件列名  
源文件的栏位名称，以英文逗号分割（结尾不能是逗号）,必须保证列数和文件内容一致.
创建hive外表（临时表）所用表列名  

7. 字段映射关系  
hive表列名,以英文逗号分隔,表示的列的内容顺序,需和DB列字段保持一致。决定从临时表往目的表里写的字段顺序。日期和常量需要用中括号包起来，例如：[${YYYYMMDD}], [\'test\']
 
13. 任务超时（分钟）  
暂时没有启用

14. beeline输出格式  
连接的源服务器，对应的beeline 输出格式

15. 脏数据阀值   
允许出错的百分比，20代表允许有20%的数据可以读或者写失败，0代表不允许有任何数据读写失败。读写各算一次失败。

16. 数据源为空是否允许成功  
无源文件或入库记录为0时,可以指定任务为成功或失败。   
选择成功，表示无源文件或入库记录为0时 ，任务成功，反之失败。  

17. 入库为空时任务处理  
无源文件或入库记录为0时,可以指定任务为成功或失败。   
选择成功，表示无源文件或入库记录为0时 ，任务成功，反之失败。  

15. 临时存储中间结果HDFS环境  
存放临时结果的hdfs 连接地址。

16. 数据临时存放目录  
存放临时结果的hdfs上的目录，默认在/user/${portalUser}/hm/ 目录下

11. 读并发度  
读hdfs 数据的并发线程数。

12. 写并发度  
数据写入mysql的并发线程数。
 
22. map个数  
暂时没有启用

### demo
如上图所示  

### demo资源
